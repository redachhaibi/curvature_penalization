{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides Evaluations 1-4 for the geodesics in the latent space of the autoencoder. Later the results are used in sections 4.52 and 4.5.3 of my thesis.\n",
    "\n",
    "Evaluation 5 is still work in progress\n",
    "\n",
    "The benchmarks evaluates the effect of curvature regularization, measuring how far straight lines are from being the geodesics.\n",
    "\n",
    "Let us consider straight line segments $\\alpha_{ij} \\subset M \\subset \\mathbb{R}^2$ connecting pairs of points $X_i$\n",
    "and $X_j$ given by:\n",
    "$$\n",
    "\\alpha_{ij} ( t ) = t \\Phi_{\\theta}(X_i) + (1-t) \\Phi_{\\theta}(X_j) \\ .\n",
    "$$\n",
    "Let us consider the images of $\\alpha_{ij}$ through the decoder $\\Psi$. One obtains curves in $\\mathbb{R}^D$ given by:\n",
    "$$\n",
    "\\gamma_{ij} ( t ) = \\Psi(t \\Phi_{\\theta}(X_i) + (1-t) \\Phi_{\\theta}(X_j)) \\ .\n",
    "$$\n",
    "\n",
    "Namely for a curve $\\gamma_{ij}(t)$ one can consider several functionals:\n",
    "The functional computing the second derivative of $\\gamma (t)$. If $t$ were a natural parameter, the functional below would compute the norm of the acceleration of the curve along the curve. However it does not, once the metric in $M$ is not Euclidean and thus $t$ is not the natural parameter.\n",
    "\\begin{equation}\n",
    "    \\text{1. }\\widetilde E_{ij} = \\int\\limits_0^1 \\|\\gamma_{ij}'' (t)\\|^2 dt \\ .\n",
    "\\end{equation}\n",
    "The energy functional of $\\alpha_{ij} ( t )$:\n",
    "\\begin{equation}\n",
    "    \\text{2. } E_{ij} = \\int\\limits_0^1 \\|\\alpha_{ij}' (t)\\|_g^2 dt = \\int\\limits_0^1 \\|\\gamma_{ij}' (t)\\|^2 dt \\ ,\n",
    "\\end{equation}\n",
    "recall $g = J_\\Psi^* J_\\Psi$ is the pull-back of the Euclidean metric by the decoder $\\Psi$.\n",
    "\n",
    "And finally, the acceleration functional:\n",
    "\\begin{equation}\n",
    "    \\text{3. } A_{ij} = \\int\\limits_0^1 \\| \\nabla_{\\alpha_{ij}' (t)} \\alpha_{ij}' (t) \\|^2 dt \\ ,\n",
    "\\end{equation}\n",
    "All the functionals are avereged:\n",
    "\\begin{equation}\n",
    "    \\mathcal{E} = \\frac{1}{\\binom{K}{2}} \\sum\\limits_{1 \\leq i < j \\leq K} E_{ij} \\ .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import yaml\n",
    "import ricci_regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../experiments/MNIST_Setting_1_config.yaml', 'r') as yaml_file:\n",
    "#with open('../../experiments/MNIST01_exp7_config.yaml', 'r') as yaml_file:\n",
    "#with open('../../experiments/Swissroll_exp4_config.yaml', 'r') as yaml_file:\n",
    "    yaml_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "violent_saving = False # if False it will not save plots\n",
    "\n",
    "#number of points to be paiwise connected by straight lines\n",
    "#K = 100 # this can go up to 300 in practice\n",
    "K = 10\n",
    "d = 2\n",
    "print(\"number of points to be paiwise connected by straight lines:\", K)\n",
    "print(\"straight lines constructed\", int(K*(K-1)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data and nn weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data loaders based on YAML configuration\n",
    "# Set the random seed for data loader reproducibility\n",
    "torch.manual_seed(yaml_config[\"data_loader_settings\"][\"random_seed\"])\n",
    "print(f\"Set random seed to: {yaml_config['data_loader_settings']['random_seed']}\")\n",
    "\n",
    "try:\n",
    "    dtype = getattr(torch, yaml_config[\"architecture\"][\"weights_dtype\"]) # Convert the string to actual torch dtype\n",
    "except KeyError:\n",
    "    dtype = torch.float32\n",
    "dict = ricci_regularization.DataLoaders.get_dataloaders(\n",
    "    dataset_config=yaml_config[\"dataset\"],\n",
    "    data_loader_config=yaml_config[\"data_loader_settings\"],\n",
    "    dtype=dtype\n",
    ")\n",
    "train_loader = dict[\"train_loader\"]\n",
    "test_loader = dict[\"test_loader\"]\n",
    "test_dataset = dict.get(\"test_dataset\")  # Assuming 'test_dataset' is a key returned by get_dataloaders\n",
    "\n",
    "print(\"Data loaders created successfully.\")\n",
    "additional_path=\"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = yaml_config[\"experiment\"][\"name\"]\n",
    "\n",
    "#Path_pictures = yaml_config[\"experiment\"][\"path\"]\n",
    "Path_pictures = additional_path + \"../experiments/\" + yaml_config[\"experiment\"][\"name\"]\n",
    "if violent_saving == True:\n",
    "    # Check and create directories based on configuration\n",
    "    if not os.path.exists(Path_pictures):  # Check if the picture path does not exist\n",
    "        os.mkdir(Path_pictures)  # Create the directory for plots if not yet created\n",
    "        print(f\"Created directory: {Path_pictures}\")  # Print directory creation feedback\n",
    "    else:\n",
    "        print(f\"Directiry already exists: {Path_pictures}\")\n",
    "\n",
    "curv_w = yaml_config[\"loss_settings\"][\"lambda_curv\"]\n",
    "\n",
    "dataset_name = yaml_config[\"dataset\"][\"name\"]\n",
    "D = yaml_config[\"architecture\"][\"input_dim\"]\n",
    "# D is the dimension of the dataset\n",
    "if dataset_name in [\"MNIST01\", \"Synthetic\"]:\n",
    "    # k from the JSON configuration file is the number of classes\n",
    "    #k = yaml_config[\"dataset\"][\"k\"]\n",
    "    k = len(yaml_config[\"dataset\"][\"selected_labels\"])\n",
    "    selected_labels = yaml_config[\"dataset\"][\"selected_labels\"]\n",
    "elif dataset_name == \"MNIST\":\n",
    "    k = 10\n",
    "print(\"Experiment name:\", experiment_name)\n",
    "print(\"Plots saved at:\", Path_pictures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(test_dataset)\n",
    "\n",
    "first_benchmark_data,rest = torch.utils.data.random_split(test_dataset, [K, l - K])\n",
    "\n",
    "#reformating\n",
    "first_benchmark_data = torch.stack([first_benchmark_data[i][0] for i in range(len(first_benchmark_data))])\n",
    "first_benchmark_data = first_benchmark_data.reshape(-1,D)\n",
    "\n",
    "d = yaml_config[\"architecture\"][\"latent_dim\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../../experiments/Swissroll_exp0_config.yaml', 'r') as yaml_file:\n",
    "#    yaml_config = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "\n",
    "torus_ae, Path_ae_weights = ricci_regularization.DataLoaders.get_tuned_nn(config=yaml_config, additional_path = additional_path)\n",
    "\n",
    "torus_ae = torus_ae.to(\"cpu\")\n",
    "\n",
    "print(f\"AE weights loaded successfully from {Path_ae_weights}.\")\n",
    "\n",
    "encoder = torus_ae.encoder_torus\n",
    "decoder = torus_ae.decoder_torus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the curve $\\gamma (t)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluates gamma at t between x and y\n",
    "def gamma (t, x, y, decoder):\n",
    "    return decoder( y * t + x * ( 1 - t ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\gamma ' ( x ) \\approx \\frac{ \\gamma ( x + h ) - \\gamma ( x - h ) }{ 2 h }\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first derivative of gamma that connects x and y at t \n",
    "def gamma_prime(t, h,  x, y, decoder):\n",
    "    return (gamma(t+h, x, y,decoder) - gamma(t-h, x, y, decoder))/( 2 * h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\gamma''(x) \\approx \\frac{\\gamma(x+h) - 2\\gamma(x) + \\gamma(x-h)}{h^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_second(t, h,  x, y, decoder ):\n",
    "    return (gamma(t+h, x, y, decoder = decoder) - 2*gamma(t, x, y, decoder = decoder) + gamma(t-h, x, y, decoder = decoder))/(h**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\n",
    " \\widetilde E_{ij} \\approx \\frac{1}{2 (n-1)} \\sum\\limits_{k=0}^{n-2} \\left( \\| \\gamma''_{ij}(t_k) \\|^2 + \\| \\gamma''_{ij}(t_{k+1}) \\|^2 \\right)\n",
    " $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_tilde(x_i,x_j,n_partition, decoder):\n",
    "    n = n_partition\n",
    "    segment_partition = ( 1 / (n-1) ) * torch.arange(n, dtype=torch.float32)\n",
    "    segment_partition_dim_d = segment_partition.repeat(d,1).T\n",
    "    gamma_second_array = gamma_second (segment_partition_dim_d,\n",
    "              h = 1/n, x = x_i, y = x_j,\n",
    "              decoder = decoder)\n",
    "    gamma_second_norm_array = gamma_second_array.norm(dim=1)\n",
    "    E_ij = ( 0.5 / ( n - 1 ) ) * torch.sum( (gamma_second_norm_array[:-1]**2 + gamma_second_norm_array[1:]**2) )\n",
    "    # return E_ij.item() doesnot work with vmap\n",
    "    return E_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    " E_{ij} \n",
    " &\\approx \\sum_{k=0}^{N} h \\left\\| \\frac{ \\gamma(kh+h) - \\gamma(kh-h)}{ 2 h } \\right\\|_2^2 \\\\\n",
    " &= \\frac{1}{4 h} \\sum_{k=0}^{N} \\left\\| \\gamma(kh+h) - \\gamma(kh-h) \\right\\|_2^2\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_gamma(t, h,  x, y, decoder):\n",
    "    return (gamma(t+h, x, y, decoder) - gamma(t-h, x, y, decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E(x_i,x_j,n_partition, decoder):\n",
    "    # n_partition is number of points in partition for \n",
    "    # the integral approximation by its Riemann sum\n",
    "    n = n_partition\n",
    "    segment_partition = (1/(n-1))*torch.arange(n,dtype=torch.float32)\n",
    "    segment_partition_dim_d = segment_partition.repeat(d,1).T\n",
    "    delta_gamma_array = delta_gamma (segment_partition_dim_d,\n",
    "              h = 1/ (n + 1), x = x_i, y = x_j,\n",
    "              decoder = decoder)\n",
    "    delta_gamma_array_norm_array = delta_gamma_array.norm(dim=1)\n",
    "    # h = 1/ (N + 1)\n",
    "    E_ij = 0.25 * (n + 1) * torch.sum( delta_gamma_array_norm_array ** 2 )\n",
    "    # return E_ij.item() doesnot work with vmap\n",
    "    return E_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A_{ij} = \\int_0^1 \\| \\gamma''_T \\|^2 dt\n",
    "\\approx \\frac{1}{N - 1} \\sum_{k = 1}^N \\| Proj_{ \\langle v_1, v_2 \\rangle }( \\gamma(kh+h) - 2\\gamma(kh) + \\gamma(kh-h) ) \\|_2^2 \\ ,\n",
    "$$\n",
    "where $v_1$ and $v_2$ are orthogonormal basis of $\\langle d \\Psi e_1 , d \\Psi e_2 \\rangle$ at point $kh$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma(x+h) - 2\\gamma(x) + \\gamma(x-h)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_gamma_second(t, h,  x, y, decoder):\n",
    "    gamma_right = gamma(t+h, x, y, decoder)\n",
    "    gamma_left = gamma(t - h, x, y, decoder)\n",
    "    gamma_central = gamma(t, x, y, decoder)\n",
    "    return gamma_left + gamma_right - 2 * gamma_central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite using delta_second of the gamma\n",
    "def A(x_i,x_j,n_partition, decoder):\n",
    "    # n_partition is number of points in partition for \n",
    "    # the integral approximation by its Riemann sum\n",
    "    n = n_partition\n",
    "    segment_partition = (1/(n-1))*torch.arange(n, dtype=torch.float32)\n",
    "    segment_partition_dim_d = segment_partition.repeat(d,1).T\n",
    "    alpha_array = x_j * segment_partition_dim_d + x_i * ( 1 - segment_partition_dim_d )\n",
    "\n",
    "    dPsi = torch.func.vmap( torch.func.jacfwd(decoder)) \n",
    "    Q,_ = dPsi(alpha_array).qr()\n",
    "    gamma_second_array = gamma_second (segment_partition_dim_d,\n",
    "                h = 1/(n - 1), x = x_i, y = x_j,\n",
    "                decoder = decoder)\n",
    "    # parallel multiplication of batches of matrices Q and vector gamma_second \n",
    "    # evaluated at intermediate points od segment partition\n",
    "    A_ij = (1 / ( n - 1 )) * ( torch.matmul(Q.transpose(-1,-2), gamma_second_array.unsqueeze(-1))**2  ).sum()\n",
    "    return A_ij.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def make_pairs(batch_of_points):\n",
    "    start_points_list = []\n",
    "    end_points_list = []\n",
    "    for i in range(K):\n",
    "        for j in range(i+1,K):\n",
    "            start_points_list.append(batch_of_points[i].unsqueeze(0))\n",
    "            end_points_list.append(batch_of_points[j].unsqueeze(0))\n",
    "    start_points = torch.cat(start_points_list, dim = 0)\n",
    "    end_points = torch.cat(end_points_list)\n",
    "    return start_points, end_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=1e-2\n",
    "(1/h**4)*delta_gamma_second(t=0,x=torch.tensor([[0., 0.]]), y =torch.tensor([[1., 1.]]), decoder=decoder, h=h).norm()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instable once h < 1.e-3\n",
    "gamma_second(t=0,x=torch.tensor([[0., 0.]]), y =torch.tensor([[1., 1.]]), decoder=decoder, h=h).norm()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : torch.sin(x)\n",
    "def ddf (x,h):\n",
    "    return f(x+h) - 2*f(x) +  f(x-h)\n",
    "def f_second (x,h):\n",
    "    return (f(x+h) - 2*f(x) +  f(x-h))/(h**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = torch.zeros(1) + 0.2\n",
    "print(f\"ddf({x_0.item()}) using h = 1.e-3\")\n",
    "print(ddf(x_0, h = 1.e-3).item())\n",
    "print(f\"ddf({x_0.item()}) using h = 1.e-5\")\n",
    "print(ddf(x_0, h = 1.e-5).item())\n",
    "\n",
    "print(f\"f''({x_0.item()}) using h = 1.e-3\")\n",
    "print(f_second(x_0, h = 1.e-3).item())\n",
    "print(f\"f''({x_0.item()}) uning h = 1.e-5\")\n",
    "print(f_second(x_0, h = 1.e-5).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the curves $\\alpha_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# define a square box where points are sampled\n",
    "square_side = 2*torch.pi\n",
    "center_of_square = torch.zeros(2)\n",
    "\n",
    "random_points_latent_space = square_side * ( torch.rand(K, 2) - 0.5 )  + center_of_square\n",
    "# Convert the tensor to a numpy array\n",
    "points_np = random_points_latent_space.numpy()\n",
    "\n",
    "# Plot the points\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(points_np[:, 0], points_np[:, 1], c='blue', marker='o', edgecolor='k')\n",
    "plt.title('Random Points in Latent Space')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same Random points (uniformly distributed) not depending on the encoder:\n",
    "\n",
    "start_points_latent_space, end_points_latent_space = make_pairs(random_points_latent_space)\n",
    "# Random points depending on the encoder:\n",
    "#start_points, end_points = make_pairs(first_benchmark_data)\n",
    "#start_points_latent_space = encoder(start_points).detach()\n",
    "#end_points_latent_space = encoder(end_points).detach()\n",
    "\"\"\"\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(start_points_latent_space[:, 0], start_points_latent_space[:, 1], color='blue', label='Start Points')\n",
    "plt.scatter(end_points_latent_space[:, 0], end_points_latent_space[:, 1], color='red', label='End Points')\n",
    "for start, end in zip(start_points_latent_space, end_points_latent_space):\n",
    "    plt.plot([start[0], end[0]], [start[1], end[1]], 'k--', color='blue')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title(r'Lines $\\alpha_{ij}$ Connecting Points in the latent space')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vmap vectorization and computation of functionals $\\widetilde E$, $E$ and $A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_tilde_vmap = torch.func.vmap(E_tilde)\n",
    "E_vmap = torch.func.vmap(E)\n",
    "A_vmap = torch.func.vmap(A)\n",
    "\n",
    "#computing Functiolnals \n",
    "n_partition = 100\n",
    "\n",
    "E_tilde_array = E_tilde_vmap (start_points_latent_space, end_points_latent_space, n_partition=n_partition, decoder = decoder)\n",
    "Energy_array = E_vmap (start_points_latent_space, end_points_latent_space, n_partition=n_partition, decoder = decoder)\n",
    "Acceleration_array = A_vmap (start_points_latent_space, end_points_latent_space, n_partition=n_partition, decoder = decoder)\n",
    "\n",
    "#E_tilde_array = E_tilde_vmap(encoder(start_points),encoder(end_points),n_partition=100,decoder = decoder)\n",
    "#Energy_array = E_vmap(encoder(start_points),encoder(end_points),n_partition=100,decoder = decoder)\n",
    "#Distance_ls_pairwize_array_no_curv_pen = (encoder(end_points) - encoder(start_points)).norm(dim=1)\n",
    "#Distance_RD_pairwize_array_no_curv_pen = (end_points - start_points).norm(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms and statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(samples, samples_name: str, n_bins = None, xlim = None, show_title_labels = False):\n",
    "    \n",
    "    samples = samples.detach()  # Detach from the computation graph\n",
    "    N = len(samples)  # Number of samples (straight lines)\n",
    "    if n_bins == None:\n",
    "        num_bins = 1 + math.ceil( math.log2(N) ) # Sturge's rule\n",
    "    else:\n",
    "        num_bins = n_bins\n",
    "    # num_bins = 30 was used before\n",
    "\n",
    "    # Create a histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(samples.detach(), bins=num_bins, edgecolor='black', alpha=0.7, density=False)\n",
    "\n",
    "    # Add labels and title\n",
    "    #plt.xlabel(f'${samples_name}_{{ij}}$ Values')\n",
    "    if show_title_labels:\n",
    "        plt.title(f'{samples_name} values')\n",
    "        plt.ylabel('Frequency')\n",
    "    if xlim != None:\n",
    "        plt.xlim(0, xlim)\n",
    "\n",
    "    # Show grid for better readability\n",
    "    plt.grid(True)\n",
    "    file_name = Path_pictures+f\"/Histogram_{samples_name}_ij_{experiment_name}.pdf\"\n",
    "    plt.savefig(file_name, bbox_inches='tight', format = \"pdf\")\n",
    "    print(\"Histogram saved at:\", file_name)\n",
    "    return plt\n",
    "\n",
    "def statistical_analysis(samples: torch.tensor, samples_name: str):\n",
    "    # Assuming the necessary imports and variable definitions are here\n",
    "    samples = samples.detach()  # Detach from the computation graph\n",
    "    N = len(samples)  # Number of samples (straight lines)\n",
    "    mean_value = samples.mean().item()  # Mean value of the energy functional\n",
    "    std_dev = torch.std(samples).item()  # Standard deviation\n",
    "    SE = std_dev / math.sqrt(N)  # Standard error (SE)\n",
    "    # printing here\n",
    "    print(\"Number of straight lines(samples):\", N)\n",
    "    print(f\"Mean value of {samples_name}: {mean_value:.3f}\")\n",
    "    print(f\"Std of {samples_name}: {std_dev:.3f}\")\n",
    "    print(f\"Standard error of mean (SE): {SE:.4f}\")\n",
    "    # Define the path to the output file\n",
    "    output_file_path = f\"{Path_pictures}/statistical_analysys_{samples_name}_{experiment_name}.txt\"\n",
    "\n",
    "    # Save the results to the text file\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        f.write(f\"Number of straight lines (samples): {N}\\n\")\n",
    "        f.write(f\"Mean value of {samples_name}: {mean_value:.3f}\\n\")\n",
    "        f.write(f\"Standard deviation of {samples_name}: {std_dev:.3f}\\n\")\n",
    "        f.write(f\"Standard error of the mean (SE): {SE:.4f}\\n\")\n",
    "\n",
    "    print(f\"Results saved to {output_file_path}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionals $\\widetilde E_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_E_tilde = plot_histogram(samples=E_tilde_array, samples_name=\"E_tilde\",n_bins=30)\n",
    "p_E_tilde.show()\n",
    "statistical_analysis(E_tilde_array, \"E_tilde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionals $ E_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_E = plot_histogram(samples=Energy_array, samples_name=\"E\",n_bins=30)\n",
    "p_E.show()\n",
    "statistical_analysis(Energy_array, \"E\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionals $ A_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_A = plot_histogram(samples=Acceleration_array, samples_name=\"A\", xlim=400e+3)\n",
    "p_A.show()\n",
    "statistical_analysis(Acceleration_array, \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tensors to a single file\n",
    "torch.save({'E^1': E_tilde_array, 'E^2': Energy_array, 'E^3': Acceleration_array}, Path_pictures+'/3Functionals.pt')\n",
    "print(\"results saved at:\", Path_pictures+'3Functionals.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compute energy rations between the geodesics $\\beta_{ij}$ and straigt line segments $\\alpha_{ij}$. Note that:\n",
    "$$\n",
    "R_{ij}^{(1)}\n",
    "= \\frac{\\int_0^1 \\| \\dot \\beta_{ij}(t)\\|_g^2 dt}{\\int_0^1 \\| \\dot \\gamma_{ij} (t)\\|_2^2 dt} \n",
    "= \\frac{\\int_0^1 \\| \\dot \\beta_{ij}(t)\\|_g^2 dt}{\\int_0^1 \\| \\dot \\alpha_{ij} (t)\\|_g^2 dt} \\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy ratios: $ \\frac{\\int_0^1 \\| \\dot \\beta_{ij}(t)\\|_g^2 dt}{\\int_0^1 \\| \\dot \\gamma_{ij} (t)\\|_2^2 dt} \\ . $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A demo for two points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ricci_regularization import NumericalGeodesics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodesic_solver = NumericalGeodesics(n_max=7, step_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "optimization_device = \"cuda\"\n",
    "\n",
    "torus_ae = torus_ae.to(optimization_device)\n",
    "# alphas are linear segments, betas are geodesics\n",
    "alpha_array, beta_array = geodesic_solver.computeGeodesicInterpolationBatch(generator=torus_ae.decoder_torus,\n",
    "                                                  m1_batch=start_points_latent_space, \n",
    "                                                  m2_batch=end_points_latent_space,\n",
    "                                                  epochs=num_epochs, device=optimization_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing energies\n",
    "torus_ae = torus_ae.to(\"cpu\")\n",
    "alpha_energies_array = ricci_regularization.compute_energy(alpha_array, decoder=torus_ae.decoder_torus,\n",
    "                                                           reduction=\"none\")\n",
    "beta_energies_array = ricci_regularization.compute_energy(beta_array, decoder=torus_ae.decoder_torus,\n",
    "                                                           reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving energy ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(beta_energies_array.detach(), Path_pictures+'/geodesic_energy_array.pt')\n",
    "print(\"results saved at:\", Path_pictures+'/geodesic_energy_array.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plot_histogram(beta_energies_array / alpha_energies_array, samples_name=\"Naive_energy_ratio\")\n",
    "p.show()\n",
    "statistical_analysis(beta_energies_array / alpha_energies_array, samples_name=\"Naive_energy_ratio\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ricci (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
